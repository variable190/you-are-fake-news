{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of concept Neural Network #\n",
    "\n",
    "## Load tweets and labels ##\n",
    "\n",
    "Create vocabulary dictionary with unique indexes\n",
    "\n",
    "Tokenise tweets with unique indexes\n",
    "\n",
    "*This network only takes one instance of each word per tweet,\n",
    "for example the tweet \"test test test\" would be passed to the\n",
    "network as a single instance of the word \"test\". The main \n",
    "product will consider multiple instances of a word*\n",
    "\n",
    "The dataset is made up of tweets that link to The Guardain\n",
    "(Labelled 1) and tweets that link to The Daily Mail (labelled\n",
    "0).  This is sentiment analysis of sorts, however, instead\n",
    "of predicting the senitment of the tweets it predicts \n",
    "whether the twitter account is sharing left wing or right\n",
    "wing news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readlines into list of tweets and labels\n",
    "with open('tweets.txt') as f:\n",
    "    raw_tweets = f.readlines()\n",
    "\n",
    "with open('labels.txt') as f:\n",
    "    raw_labels = f.readlines()\n",
    "\n",
    "# split list of tweets in 2D set {tweets,words}\n",
    "tokens = list(map(lambda x:set(x.split(\" \")),raw_tweets))\n",
    "\n",
    "# create vocabulary list: compile tweets into 1D set {words} \n",
    "# removing duplicates and nulls('') then convert to a list\n",
    "vocab = set()\n",
    "for tweet in tokens:\n",
    "    for word in tweet:\n",
    "        if(len(word)>0):\n",
    "            vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "# enumerate list of words to give unique numerical key to each word\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "\n",
    "# Create list of tweets with unique numerical key inplace of words\n",
    "# pass from list to set and back to list to remove duplicates\n",
    "input_dataset = list()\n",
    "for tweet in tokens:\n",
    "    tweet_indices = list()\n",
    "    for word in tweet:\n",
    "        try:\n",
    "            tweet_indices.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(list(set(tweet_indices)))\n",
    "    \n",
    "# Create list of output targets\n",
    "target_dataset = list()\n",
    "for label in raw_labels:\n",
    "    if label == '1\\n':\n",
    "        target_dataset.append(1)\n",
    "    else:\n",
    "        target_dataset.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Run Network ##\n",
    "\n",
    "This a simple network with an embedding layer as the input\n",
    "layer, one hidden layer of 100 nodes and a binary output layer.\n",
    "Each layer is densely connected however only the indexes of the \n",
    "current tweet are considered on each cycle. The sigmoid activation\n",
    "function is used for both the hidden and output layer. The \n",
    "learning rate is set at 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:1 Progress:100.0% Training Accuracy:84.06%\n",
      "Iter:2 Progress:100.0% Training Accuracy:87.49%\n",
      "Iter:3 Progress:100.0% Training Accuracy:89.25%\n",
      "Iter:4 Progress:100.0% Training Accuracy:90.42%\n",
      "Iter:5 Progress:100.0% Training Accuracy:91.30%\n",
      "Test Accuracy:90.71%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# function to convert input to value between 0 and 1\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "alpha, iterations = (0.01, 5)\n",
    "hidden_size = 100\n",
    "\n",
    "# Create matrix with rows equal amount of unique words in all \n",
    "# tweets by hidden layer of columns\n",
    "weights_0_1 = 0.2*np.random.random((len(vocab),hidden_size)) - 0.1\n",
    "# Create matrix with hidden layer of rows and 1 column\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size,1)) - 0.1\n",
    "\n",
    "\n",
    "correct,total = (0,0)\n",
    "for iter in range(iterations):\n",
    "    # train on first 80,000 tweets\n",
    "    for i in range(len(input_dataset)-20000):\n",
    "        \n",
    "        # first tweet and outcome set to x and y\n",
    "        x,y = (input_dataset[i],target_dataset[i])\n",
    "        # sum matrix columns of rows representing words present in \n",
    "        # current tweet then apply sigmoid\n",
    "        layer_1 = sigmoid(np.sum(weights_0_1[x],axis=0)) \n",
    "        # matrix multiplication of hidden layer and second set of \n",
    "        # weights then apply sigmoid\n",
    "        layer_2 = sigmoid(np.dot(layer_1,weights_1_2))\n",
    "\n",
    "        # calculate delta by taking actual from prediction and \n",
    "        # back propigate to calculate layer 1 delta\n",
    "        layer_2_delta = layer_2 - y\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T)\n",
    "\n",
    "        # subtract delta from linear layer on rows that were \n",
    "        # present in current review\n",
    "        weights_0_1[x] -= layer_1_delta * alpha\n",
    "        # multiply delta (1) into layer 1 (100) = (100,1) subtract \n",
    "        # from 2nd linear layer\n",
    "        weights_1_2 -= np.outer(layer_1,layer_2_delta) * alpha\n",
    "        \n",
    "        # convert layer_2_delta to positive and if its below 0.5 \n",
    "        # increment correct\n",
    "        if(np.abs(layer_2_delta) < 0.5):\n",
    "            correct += 1\n",
    "            \n",
    "        total += 1\n",
    "        \n",
    "        # print progress every 10 increments\n",
    "        if(i % 10 == 0):\n",
    "            progress = str((i/(len(input_dataset)-20010))*100)\n",
    "            sys.stdout.write('\\rIter:'+str(iter+1)+' Progress:'+\n",
    "                             progress[0:2]+progress[2:5] +\n",
    "                             '% Training Accuracy:'+ \n",
    "                             str(correct/float(total))[2:4]+'.'+\n",
    "                             str(correct/float(total))[4:6]+ '%')\n",
    "            time.sleep(0.001)\n",
    "    print()\n",
    "    \n",
    "# repeat for test dataset\n",
    "correct,total = (0,0)\n",
    "for i in range(len(input_dataset)-20000,len(input_dataset)):\n",
    "\n",
    "    x = input_dataset[i]\n",
    "    y = target_dataset[i]\n",
    "\n",
    "    layer_1 = sigmoid(np.sum(weights_0_1[x],axis=0))\n",
    "    layer_2 = sigmoid(np.dot(layer_1,weights_1_2))\n",
    "    \n",
    "    if(np.abs(layer_2 - y) < 0.5):\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(\"Test Accuracy:\" + str(correct/float(total))[2:4]+'.'+\n",
    "      str(correct/float(total))[4:6] + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions ##\n",
    "\n",
    "After five iterations the the training and test accuracy are\n",
    "both around 91%.  This suggests that the network is generalising\n",
    "well.  To test if this is the case the network will be passed\n",
    "a set of ten tweets collected from two weeks after the dataset.\n",
    "These tweets should have no likeness with regards to subject as\n",
    "the news cycle should have moved on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "Prediction: 0.9691711710709275\n",
      "Actual: 1\n",
      "\n",
      "2.\n",
      "Prediction: 0.9985246924457338\n",
      "Actual: 1\n",
      "\n",
      "3.\n",
      "Prediction: 0.9984254692500265\n",
      "Actual: 1\n",
      "\n",
      "4.\n",
      "Prediction: 0.013475537512806928\n",
      "Actual: 0\n",
      "\n",
      "5.\n",
      "Prediction: 0.36792776568256974\n",
      "Actual: 0\n",
      "\n",
      "6.\n",
      "Prediction: 0.0011052837807767613\n",
      "Actual: 0\n",
      "\n",
      "7.\n",
      "Prediction: 0.07300761724035176\n",
      "Actual: 1\n",
      "\n",
      "8.\n",
      "Prediction: 0.9999953671271974\n",
      "Actual: 1\n",
      "\n",
      "9.\n",
      "Prediction: 0.0002536731724130849\n",
      "Actual: 0\n",
      "\n",
      "10.\n",
      "Prediction: 0.026135439533821286\n",
      "Actual: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Readlines into list of tweets and labels\n",
    "f = open('predict_tweets.txt')\n",
    "raw_predict_tweets = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('predict_labels.txt')\n",
    "raw_predict_labels = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# split list of tweets in 2D set {tweets,words}\n",
    "predict_tokens = list(map(lambda x:set(x.split(\" \")),raw_predict_tweets))\n",
    "\n",
    "# Create list of tweets with unique numerical key inplace of words\n",
    "# pass from list to set and back to list to remove duplicates\n",
    "predict_input_dataset = list()\n",
    "for tweet in predict_tokens:\n",
    "    tweet_indices = list()\n",
    "    for word in tweet:\n",
    "        try:\n",
    "            tweet_indices.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    predict_input_dataset.append(list(set(tweet_indices)))\n",
    "    \n",
    "# Create list of output targets\n",
    "predict_target_dataset = list()\n",
    "for label in raw_predict_labels:\n",
    "    if label == '1\\n':\n",
    "        predict_target_dataset.append(1)\n",
    "    else:\n",
    "        predict_target_dataset.append(0)\n",
    "        \n",
    "# Construct network with trained weights and test on\n",
    "# prediction dataset\n",
    "predictions = list()\n",
    "for i in range(len(predict_input_dataset)):\n",
    "\n",
    "    x = predict_input_dataset[i]\n",
    "\n",
    "    layer_1 = sigmoid(np.sum(weights_0_1[x],axis=0))\n",
    "    layer_2 = sigmoid(np.dot(layer_1,weights_1_2))\n",
    "    \n",
    "    predictions.append(np.abs(layer_2 - y))\n",
    "for i in range(len(predictions)):\n",
    "    print(str(i+1)+'.\\nPrediction: '+str(predictions[i][0])+\n",
    "          '\\nActual: '+str(predict_target_dataset[i])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion ##\n",
    "\n",
    "Other than the seventh prediction the network was correct\n",
    "and other than the fifth there is very little loss. This\n",
    "suggest that networks can succesfully predict beyond\n",
    "sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
