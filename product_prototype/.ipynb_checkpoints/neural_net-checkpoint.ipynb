{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readlines into list of tweets and labels\n",
    "f = open('tweets.txt')\n",
    "raw_tweets = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('labels.txt')\n",
    "raw_labels = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# split list of tweets in 2D set {tweets,words}\n",
    "tokens = list(map(lambda x:set(x.split(\" \")),raw_tweets))\n",
    "\n",
    "# create vocabulary list: compile tweets into 1D set {words} removing duplicates and nulls('') then convert to a list\n",
    "vocab = set()\n",
    "for tweet in tokens:\n",
    "    for word in tweet:\n",
    "        if(len(word)>0):\n",
    "            vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "# enumerate list of words to give unique numerical key to each word\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "\n",
    "# Create list of tweets with unique numerical key inplace of words\n",
    "# pass from list to set and back to list to remove duplicates\n",
    "input_dataset = list()\n",
    "for tweet in tokens:\n",
    "    tweet_indices = list()\n",
    "    for word in tweet:\n",
    "        try:\n",
    "            tweet_indices.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(list(set(tweet_indices)))\n",
    "    \n",
    "# Create list of output targets\n",
    "target_dataset = list()\n",
    "for label in raw_labels:\n",
    "    if label == '1\\n':\n",
    "        target_dataset.append(1)\n",
    "    else:\n",
    "        target_dataset.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0 Progress:100.0% Training Accuracy:83.90%\n",
      "Iter:1 Progress:100.0% Training Accuracy:87.42%\n",
      "Test Accuracy:90.12%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# function to convert input to value between 0 and 1\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "alpha, iterations = (0.01, 2)\n",
    "hidden_size = 100\n",
    "\n",
    "# Create matrix with rows equal amount of unique words in all tweets by hidden layer of columns\n",
    "weights_0_1 = 0.2*np.random.random((len(vocab),hidden_size)) - 0.1\n",
    "# Create matrix with hidden layer of rows and 1 column\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size,1)) - 0.1\n",
    "\n",
    "\n",
    "correct,total = (0,0)\n",
    "for iter in range(iterations):\n",
    "    # train on first 80,000 tweets\n",
    "    for i in range(len(input_dataset)-20000):\n",
    "        \n",
    "        # first tweet and outcome set to x and y\n",
    "        x,y = (input_dataset[i],target_dataset[i])\n",
    "        # sum matrix columns of rows representing words present in current tweet then apply sigmoid\n",
    "        layer_1 = sigmoid(np.sum(weights_0_1[x],axis=0)) \n",
    "        # matrix multiplication of hidden layer and second set of weights then apply sigmoid\n",
    "        layer_2 = sigmoid(np.dot(layer_1,weights_1_2))\n",
    "\n",
    "        # calculate delta by taking actual from prediction and back propigate to calculate layer 1 delta\n",
    "        layer_2_delta = layer_2 - y\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T)\n",
    "\n",
    "        # subtract delta from linear layer on rows that were present in current review\n",
    "        weights_0_1[x] -= layer_1_delta * alpha\n",
    "        # multiply delta (1) into layer 1 (100) = (100,1) subtract from 2nd linear layer\n",
    "        weights_1_2 -= np.outer(layer_1,layer_2_delta) * alpha\n",
    "        \n",
    "        # convert layer_2_delta to positive and if its below 0.5 increment correct\n",
    "        if(np.abs(layer_2_delta) < 0.5):\n",
    "            correct += 1\n",
    "            \n",
    "        total += 1\n",
    "        \n",
    "        # print progress every 10 increments\n",
    "        if(i % 10 == 0):\n",
    "            progress = str((i/(len(input_dataset)-20010))*100)\n",
    "            sys.stdout.write('\\rIter:'+str(iter)+' Progress:'+progress[0:2]+progress[2:5]\\\n",
    "                             +'% Training Accuracy:'+ str(correct/float(total))[2:4]+'.'+str(correct/float(total))[4:6]\\\n",
    "                             + '%')\n",
    "            time.sleep(0.001)\n",
    "    print()\n",
    "    \n",
    "# repeat for test dataset\n",
    "correct,total = (0,0)\n",
    "for i in range(len(input_dataset)-20000,len(input_dataset)):\n",
    "\n",
    "    x = input_dataset[i]\n",
    "    y = target_dataset[i]\n",
    "\n",
    "    layer_1 = sigmoid(np.sum(weights_0_1[x],axis=0))\n",
    "    layer_2 = sigmoid(np.dot(layer_1,weights_1_2))\n",
    "    \n",
    "    if(np.abs(layer_2 - y) < 0.5):\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(\"Test Accuracy:\" + str(correct/float(total))[2:4]+'.'+str(correct/float(total))[4:6] + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readlines into list of tweets and labels\n",
    "f = open('predict_tweets.txt')\n",
    "raw_predict_tweets = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('predict_labels.txt')\n",
    "raw_predict_labels = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# split list of tweets in 2D set {tweets,words}\n",
    "predict_tokens = list(map(lambda x:set(x.split(\" \")),raw_predict_tweets))\n",
    "\n",
    "# Create list of tweets with unique numerical key inplace of words\n",
    "# pass from list to set and back to list to remove duplicates\n",
    "predict_input_dataset = list()\n",
    "for tweet in predict_tokens:\n",
    "    tweet_indices = list()\n",
    "    for word in tweet:\n",
    "        try:\n",
    "            tweet_indices.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    predict_input_dataset.append(list(set(tweet_indices)))\n",
    "    \n",
    "# Create list of output targets\n",
    "predict_target_dataset = list()\n",
    "for label in raw_predict_labels:\n",
    "    if label == '1\\n':\n",
    "        predict_target_dataset.append(1)\n",
    "    else:\n",
    "        predict_target_dataset.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0.9142985520695054\n",
      "Actual: 1\n",
      "\n",
      "Prediction: 0.9832039372539713\n",
      "Actual: 1\n",
      "\n",
      "Prediction: 0.9957273213632801\n",
      "Actual: 1\n",
      "\n",
      "Prediction: 0.14225121367988078\n",
      "Actual: 0\n",
      "\n",
      "Prediction: 0.44059612418755334\n",
      "Actual: 0\n",
      "\n",
      "Prediction: 0.011367065330068971\n",
      "Actual: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# repeat for test dataset\n",
    "predictions = list()\n",
    "for i in range(len(predict_input_dataset)):\n",
    "\n",
    "    x = predict_input_dataset[i]\n",
    "\n",
    "    layer_1 = sigmoid(np.sum(weights_0_1[x],axis=0))\n",
    "    layer_2 = sigmoid(np.dot(layer_1,weights_1_2))\n",
    "    \n",
    "    predictions.append(np.abs(layer_2 - y))\n",
    "for i in range(len(predictions)):\n",
    "    print('Prediction: ' + str(predictions[i][0]) + '\\nActual: ' + str(predict_target_dataset[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
